---
layout: post
title: Journal Impact Factors Are Behind Black Boxes: “Show me the data” by Mike Rossner, Heather Van Epps, & Emma Hill 2007.
---

![A cartoon showing wolves questioning if howling at the moon has any impact.](/images/Rossner2007.png)

"Impact factor" is a term that aims to do what it says on the tin; it's a rating of the impact of an article.
Impact factor governs so much of a scientists career.
It is used to determine their success in a given post, their right to a grant or future job, and on a more day-to-day level it gives them bragging rights.
In a quest to increase the impact factor of a paper, researchers often aim to get their work published in journals that are not always the most appropriate.
Instead, they would rather publish in a journal with the highest "impact factor" that will consider their paper.

Impact factor is supposedly a metric that captures the academic importance of a journal.
It is reported as being the number of citations each article gets in the previous 2 years.
[Wikipedia explains impact factor](https://en.wikipedia.org/wiki/Impact_factor) very thoroughly, and I wouldn't want to repeat all of that here.

Rossner *et al.* believe that the impact factor is not exactly as straight forward as implied.
They touch on some very troubling issues that should undermine the authority of the journal impact factor.
  - The numerator of the impact factor includes different article types to the denominator.
  This means that articles can be factored into increasing the numerator without increasing the denominator, for example front matter can validly gather citations, but does not count as an article, allowing the journal to accumulate "free" citations.
  - Articles, reviews, and front matter are curated by hand by Thomson Scientific. These designations were found to be routinely incorrect when the group purchased data from Thomson Scientific for scrutiny.
  - These designations can be negotiated in a non-transparent way. This has allowed journals to vastly reduce their article number inflating the citations per article.
  - Retracted articles still count.
  - The mean average is skewed by blockbuster publications, meaning the impact factor does not reflect the majority of articles in a journal.


  > In a self-analysis of their 2005 impact factor, Nature noted that 89% of their citations came from only 25% of the papers published.

  - The data you can purchase from Thomson Scientific is **not** the data they use to calculate the impact factor.

  > When we requested the database used to calculate the published impact factors (i.e., including the erroneous records), Thomson Scientific sent us a second database. But these data still did not match the published impact factor data. This database appeared to have been assembled in an ad hoc manner to create a facsimile of the published data that might appease

I find it astonishing scientists tolerate such an unscientific method of deciding the scientific quality and validity of individuals and groups.

The article ends in an optimistic tone, calling for other metrics to emerge that may usurp the opaque and ill-defined impact factor.

* * *

Rossner, M., Van Epps, H. & Hill, E. Show me the data. J. Cell Biol. 179, 1091–1092 (2007).

Retrieved from http://jcb.rupress.org/content/179/6/1091.full
